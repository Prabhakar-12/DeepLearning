{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37c21b91add6bf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m X=torch.tensor([\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.is_tensor(X))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X=torch.tensor([1,2,3,4])\n",
    "print(torch.is_tensor(X))\n",
    "s=X.storage()\n",
    "print(torch.is_storage(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe676756d07518f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m a=\u001b[43mtorch\u001b[49m.randn(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#Gives the Total Elements Present in  the Tensor... like If the matrix has size (2,3) give the Total Elements as 5\u001b[39;00m\n\u001b[32m      3\u001b[39m a=torch.randn(\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "a=torch.randn(1,2,3,4)\n",
    "#Gives the Total Elements Present in  the Tensor... like If the matrix has size (2,3) give the Total Elements as 5\n",
    "a=torch.randn(2,3,4)\n",
    "torch.numel(a)\n",
    "b=torch.randn(2,3,4,5)\n",
    "torch.numel(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae4fc2a-d36b-44bd-99f5-1e8cf2f3e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1a0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842f0992-1176-4330-9dd1-369d22290e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poioioioio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m Hidden_Layer1_Neuron1\u001b[38;5;241m=\u001b[39mNeuron(weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],bais\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     18\u001b[0m inputs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m \u001b[43mpoioioioio\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'poioioioio' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=weights\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        weighted_sum=0\n",
    "        for i in range(len(self.weights)):\n",
    "            weighted_sum+=self.weights[i]*inputs[i]\n",
    "\n",
    "        output=weighted_sum+self.bais\n",
    "        if output>=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "#Hidden Layer \n",
    "Hidden_Layer1_Neuron1=Neuron(weights=[2,3],bais=3)\n",
    "inputs=[3,4]\n",
    "poioioioio\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccddeec6-5a11-408a-8ba3-f5043196c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple neuron model\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weighted_sum += self.weights[i] * inputs[i]\n",
    "\n",
    "        # Add the bias\n",
    "        output = weighted_sum + self.bias\n",
    "        \n",
    "        # Apply an activation function (here we use a simple step function for clarity)\n",
    "        if output >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Example usage\n",
    "# Define the weights and bias for the neuron\n",
    "# Imagine we're trying to predict if a student passes a test based on\n",
    "# hours studied (input 1) and attendance rate (input 2)\n",
    "# Weight 1 (0.5) gives more importance to hours studied, Weight 2 (0.3) to attendance.\n",
    "# A bias of -2 means the student needs a certain minimum \"score\" to pass.\n",
    "neuron_weights = [0.5, 0.3]\n",
    "neuron_bias = -2.0\n",
    "\n",
    "# Create an instance of the Neuron\n",
    "my_neuron = Neuron(neuron_weights, neuron_bias)\n",
    "\n",
    "# Example 1: High hours studied (5) and high attendance (9)\n",
    "input_data_1 = [5, 9]\n",
    "output_1 = my_neuron.forward(input_data_1)\n",
    "print(f\"Inputs: {input_data_1} | Output: {output_1}\") # Output will be 1 (Pass)\n",
    "\n",
    "# Example 2: Low hours studied (2) and low attendance (4)\n",
    "input_data_2 = [2, -14]\n",
    "output_2 = my_neuron.forward(input_data_2)\n",
    "print(f\"Inputs: {input_data_2} | Output: {output_2}\") # Output will be 0 (Fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52e6e2-81f1-48b4-85da-57a969fd0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A simple neuron class using NumPy for efficiency\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum using a dot product (more efficient)\n",
    "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # Apply a simple step function as the activation\n",
    "        if weighted_sum >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# --- Building the Multi-Layer Network ---\n",
    "\n",
    "# --- Hidden Layer ---\n",
    "# Let's create two neurons for our hidden layer\n",
    "# These neurons will process the initial inputs (hours studied, attendance)\n",
    "# Neuron 1 focuses on one aspect (e.g., studying habits)\n",
    "hidden_neuron_1 = Neuron(weights=[0.5, 0.3], bias=-2.0)\n",
    "# Neuron 2 focuses on another aspect (e.g., participation and engagement)\n",
    "hidden_neuron_2 = Neuron(weights=[0.6, 0.4], bias=-3.0)\n",
    "\n",
    "# Example 1: High hours studied (5) and high attendance (9)\n",
    "input_data = np.array([5, 9])\n",
    "\n",
    "# Calculate the output of each neuron in the hidden layer\n",
    "# These outputs will be the new inputs for the next layer\n",
    "hidden_output_1 = hidden_neuron_1.forward(input_data)\n",
    "hidden_output_2 = hidden_neuron_2.forward(input_data)\n",
    "\n",
    "# Combine the outputs into a new input vector for the next layer\n",
    "hidden_layer_output = np.array([hidden_output_1, hidden_output_2])\n",
    "print(f\"Input: {input_data} | Hidden Layer Outputs: {hidden_layer_output}\")\n",
    "\n",
    "# --- Output Layer ---\n",
    "# Now, let's create a final output neuron that takes the hidden layer's outputs as its input\n",
    "# The weights here decide how much importance the network places on the outputs of the hidden neurons.\n",
    "# This neuron predicts the final outcome (Pass or Fail)\n",
    "output_neuron = Neuron(weights=[0.7, 0.8], bias=-1.0)\n",
    "final_output = output_neuron.forward(hidden_layer_output)\n",
    "print(f\"Final Output of the network: {final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077a85e-187e-45ac-947c-d93b3ff39332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=np.array(weights)\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        weighted_Sum=np.dot(self.weights,inputs)+self.bais\n",
    "        print(weighted_Sum)\n",
    "        if weighted_Sum>=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "##Building the Hidden Layers\n",
    "#First Hidden Layer\n",
    "Hidden_Layer1_neuron_1=Neuron(weights=[3,4],bais=5)\n",
    "Hidden_Layer1_neuron_2=Neuron(weights=[4,6],bais=3)\n",
    "inputs=[5,6]\n",
    "Neuron_1_Output=Hidden_Layer1_neuron_1.Forward(inputs)\n",
    "Neuron_2_Output=Hidden_Layer1_neuron_2.Forward(inputs)\n",
    "Hidden_Layer1_Output=np.array([Neuron_1_Output,Neuron_2_Output])\n",
    "print(\"The Hidden Layer 1 Output is \",Hidden_Layer1_Output)\n",
    "#Second Hidden Layer\n",
    "Hidden_Layer2_Neuron_1=Neuron(weights=[5,6],bais=3)\n",
    "Hidden_Layer2_Neuron_2=Neuron(weights=[6,7],bais=4)\n",
    "Neuron_1_Output=Hidden_Layer2_Neuron_1.Forward(Hidden_Layer1_Output)\n",
    "Neuron_2_Output=Hidden_Layer2_Neuron_2.Forward(Hidden_Layer1_Output)\n",
    "Hidden_Layer2_Output=np.array([Neuron_1_Output,Neuron_2_Output])\n",
    "print(\"The Hidden Layer 2 Output is \",Hidden_Layer2_Output)\n",
    "#Final Output is \n",
    "FinalNeuron=Neuron(weights=[5,4],bais=2)\n",
    "FinalOutput=FinalNeuron.Forward(Hidden_Layer2_Output)\n",
    "print(\"The Final Neuron output is \",FinalOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113be57-921e-425b-b936-e11992815bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=np.array(weights)\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        weighted_sum=np.dot(self.weights,inputs)+self.bais\n",
    "        if weighted_sum>=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "##Building the Hidden Layers\n",
    "#Hidden Layer 1\n",
    "Hidden_Layer1_Neuron_1=Neuron(weights=[4,5],bais=3)\n",
    "Hidden_Layer1_Neuron_2=Neuron(weights=[4,2],bais=2)\n",
    "inputs=[3,4]\n",
    "HD1_Neuron_1_Output=Hidden_Layer1_Neuron_1.Forward(inputs)\n",
    "HD1_Neuron_2_Output=Hidden_Layer1_Neuron_2.Forward(inputs)\n",
    "Hidden_Layer1_Output=np.array([HD1_Neuron_1_Output,HD1_Neuron_2_Output])\n",
    "print(\"The Hidden Layer1 Output is \",Hidden_Layer1_Output)\n",
    "#Hidden Layer 2\n",
    "Hidden_Layer2_Neuron_1=Neuron(weights=[3,4],bais=-3)\n",
    "Hidden_Layer2_Neuron_2=Neuron(weights=[-2,3],bais=-3)\n",
    "HD2_Neuron_1_Output=Hidden_Layer2_Neuron_1.Forward(Hidden_Layer1_Output)\n",
    "HD2_Neuron_2_Output=Hidden_Layer2_Neuron_2.Forward(Hidden_Layer1_Output)\n",
    "Hidden_Layer2_Output=np.array([HD2_Neuron_1_Output,HD2_Neuron_2_Output])\n",
    "print(\"The Second Hidden Layer output is \",Hidden_Layer2_Output)\n",
    "#Final Output\n",
    "FinalNeuron=Neuron(weights=[4,-5],bais=-3)\n",
    "Final_Output=FinalNeuron.Forward(Hidden_Layer2_Output)\n",
    "print(\"The Final Output is \",Final_Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c76045-aa7d-4030-af05-3364ada13313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation + derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, lr=0.1):\n",
    "        self.weights = np.array(weights, dtype=float)\n",
    "        self.bias = float(bias)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = np.array(inputs, dtype=float)\n",
    "        self.weighted_sum = np.dot(self.weights, self.inputs) + self.bias\n",
    "        self.output = sigmoid(self.weighted_sum)                                                                     \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dloss_dout):\n",
    "        d_out_d_sum = sigmoid_derivative(self.output)\n",
    "        dloss_dsum = dloss_dout * d_out_d_sum\n",
    "        dloss_dweights = dloss_dsum * self.inputs\n",
    "        dloss_dbias = dloss_dsum\n",
    "        dloss_dinputs = dloss_dsum * self.weights\n",
    "        # update params\n",
    "        self.weights -= self.lr * dloss_dweights\n",
    "        self.bias -= self.lr * dloss_dbias\n",
    "        return dloss_dinputs\n",
    "\n",
    "# create network with some chosen values\n",
    "Hidden_Layer_1 = Neuron(weights=[5, -2], bias=1, lr=0.1)\n",
    "Hidden_Layer_2 = Neuron(weights=[-3, 8], bias=-3, lr=0.1)\n",
    "Output_Neuron = Neuron(weights=[7, -9], bias=5, lr=0.1)\n",
    "\n",
    "inputs = np.array([1, -5])  # sample input\n",
    "target = 1.0                   # desired output\n",
    "\n",
    "# ---- Forward pass (before training) ----\n",
    "h1_out = Hidden_Layer_1.forward(inputs)\n",
    "h2_out = Hidden_Layer_2.forward(inputs)\n",
    "hidden_outs = np.array([h1_out, h2_out])\n",
    "final_out = Output_Neuron.forward(hidden_outs)\n",
    "\n",
    "print(\"Output BEFORE backpropagation:\", final_out)\n",
    "\n",
    "# ---- Compute loss gradient and backprop ----\n",
    "dloss_dfinal = -(target - final_out)  # dL/dOut for output neuron\n",
    "dloss_dhidden = Output_Neuron.backward(dloss_dfinal)\n",
    "Hidden_Layer_1.backward(dloss_dhidden[0])\n",
    "Hidden_Layer_2.backward(dloss_dhidden[1])\n",
    "\n",
    "# ---- Forward pass (after weight update) ----\n",
    "h1_out = Hidden_Layer_1.forward(inputs)\n",
    "h2_out = Hidden_Layer_2.forward(inputs)\n",
    "hidden_outs = np.array([h1_out, h2_out])\n",
    "final_out_after = Output_Neuron.forward(hidden_outs)\n",
    "\n",
    "print(\"Output AFTER backpropagation:\", final_out_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16f71a-5ad5-4d17-a865-1d61418623d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=np.array(weights)\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        weighted_sum=np.dot(self.weights,inputs)+self.bais\n",
    "        if weighted_sum>=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "##Building the Hidden Layers\n",
    "#Hidden Layer one\n",
    "Hidden_Layer_1_Neuron_1=Neuron(weights=[2,-4],bais=4)\n",
    "Hidden_Layer_1_Neuron_2=Neuron(weights=[4,5],bais=3)\n",
    "inputs=np.array([-4,5])\n",
    "HL1_Neuron_Output1=Hidden_Layer_1_Neuron_1.Forward(inputs)\n",
    "HL1_Neuron_Output2=Hidden_Layer_1_Neuron_2.Forward(inputs)\n",
    "Hidden_Layer_Output=np.array([HL1_Neuron_Output1,HL1_Neuron_Output2])\n",
    "print(\"The Hidden Layer 1 output is \",Hidden_Layer_Output)\n",
    "\n",
    "#Hidden Layer 2\n",
    "Hidden_Layer_2_Neuron_1=Neuron(weights=[2,-3],bais=3)\n",
    "Hidden_Layer_2_Neuron_2=Neuron(weights=[4,-5],bais=2)\n",
    "\n",
    "HL2_Neuron_Output1=Hidden_Layer_2_Neuron_1.Forward(Hidden_Layer_Output)\n",
    "HL2_Neuron_Output2=Hidden_Layer_2_Neuron_2.Forward(Hidden_Layer_Output)\n",
    "Hidden_Layer_2_Outputs=np.array([HL2_Neuron_Output1,HL2_Neuron_Output2])\n",
    "print(\"The Hidden Layer 2 output is \",Hidden_Layer_2_Outputs)\n",
    "#Final Neuron\n",
    "FinalNeuron=Neuron(weights=[3,4],bais=2)\n",
    "FinalOutput=FinalNeuron.Forward(Hidden_Layer_2_Outputs)\n",
    "print(\"The Final Output is \",FinalOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a95bfc-c046-41c7-a957-2fbbbce767ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=np.array(weights)\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        weighted_sum=np.dot(self.weights,self.inputs)+self.bais\n",
    "        if weighted_sum>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "Data=pd.read_csv(\"XORGateDataSet.csv\")\n",
    "Data.columns      \n",
    "##Building theNeuron Network\n",
    "#Hidden Layer 1\n",
    "# Hidden_Layer1_Neuron1=Neuron(weights=[3,4,4,5],bais=3)\n",
    "# Hidden_Layer1_Neuron2=Neuron(weights=[4,5,6,7],bais=2)\n",
    "# inputs=Data.iloc[:,:2]\n",
    "# Target=Data.iloc[:,-1]\n",
    "# HD1_Neuorn1_Output=Hidden_Layer1_Neuron1.Forward(inputs)\n",
    "# HD1_Neuron2_Output=Hidden_Layer1_Neuron2.Forward(inputs)\n",
    "# HD1_Outputs=np.array([HD1_Neuron1_Output,HD1_Neuron2_Output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9254c-0e1f-4d71-b6ad-9877e8b2cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d94dd-5d16-4a51-94ab-4a7a62255d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Activation functions and their derivatives\n",
    "# The sigmoid function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 2. The Neuron Class\n",
    "# This class represents a single neuron in the network.\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, lr=0.5):\n",
    "        self.weights = np.array(weights, dtype=float)\n",
    "        self.bias = float(bias)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = np.array(inputs, dtype=float)\n",
    "        self.weighted_sum = np.dot(self.weights, self.inputs) + self.bias\n",
    "        self.output = sigmoid(self.weighted_sum)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dloss_dout):\n",
    "        d_out_d_sum = sigmoid_derivative(self.output)\n",
    "        dloss_dsum = dloss_dout * d_out_d_sum\n",
    "        dloss_dweights = dloss_dsum * self.inputs\n",
    "        dloss_dbias = dloss_dsum\n",
    "        dloss_dinputs = dloss_dsum * self.weights\n",
    "        \n",
    "        # Update the neuron's parameters\n",
    "        self.weights -= self.lr * dloss_dweights\n",
    "        self.bias -= self.lr * dloss_dbias\n",
    "        \n",
    "        return dloss_dinputs\n",
    "\n",
    "# 3. Network Architecture and Data\n",
    "# Define the dataset\n",
    "# This is a small XOR gate dataset.\n",
    "XOR_inputs = np.array([[0.0, 0.0],\n",
    "                       [0.0, 1.0],\n",
    "                       [1.0, 0.0],\n",
    "                       [1.0, 1.0]])\n",
    "\n",
    "XOR_targets = np.array([0.0, 1.0, 1.0, 0.0])\n",
    "Data=pd.read_csv(\"XORGateDataSet.csv\")\n",
    "XOR_inputs1=Data[['Input_1','Input_2']]\n",
    "\n",
    "XOR_targets1=Data['Target']\n",
    "# Create the neurons for the network\n",
    "Hidden_Layer_1 = Neuron(weights=[0.5, -0.2], bias=0.1, lr=0.5)\n",
    "Hidden_Layer_2 = Neuron(weights=[0.3, 0.8], bias=-0.3, lr=0.5)\n",
    "Output_Neuron = Neuron(weights=[-0.7, 0.9], bias=0.05, lr=0.5)\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 10000\n",
    "\n",
    "# 4. Training Loop\n",
    "# The core training process that combines forward and backpropagation.\n",
    "for epoch in range(epochs):\n",
    "    # Iterate through each data sample in the dataset\n",
    "    for i in range(len(XOR_inputs)):\n",
    "        inputs = XOR_inputs[i]\n",
    "        target = XOR_targets[i]\n",
    "\n",
    "        # ---- Forward Pass ----\n",
    "        # The data flows from the input layer through the hidden layer to the output layer.\n",
    "        h1_out = Hidden_Layer_1.forward(inputs)\n",
    "        h2_out = Hidden_Layer_2.forward(inputs)\n",
    "        \n",
    "        hidden_outs = np.array([h1_out, h2_out])\n",
    "        final_out = Output_Neuron.forward(hidden_outs)\n",
    "\n",
    "        # ---- Backpropagation ----\n",
    "        # The error is propagated backward from the output layer to the hidden layer.\n",
    "        # dloss_dfinal is the derivative of the loss function (mean squared error)\n",
    "        dloss_dfinal = -(target - final_out)\n",
    "        \n",
    "        # Propagate the error back to the hidden layer neurons\n",
    "        dloss_dhidden = Output_Neuron.backward(dloss_dfinal)\n",
    "        \n",
    "        # Backpropagate the error from the hidden layer to the input layer\n",
    "        Hidden_Layer_1.backward(dloss_dhidden[0])\n",
    "        Hidden_Layer_2.backward(dloss_dhidden[1])\n",
    "\n",
    "    # Print the error every 1000 epochs to monitor progress\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        # Re-run the forward pass for the entire dataset to check the total error\n",
    "        total_error = 0\n",
    "        for i in range(len(XOR_inputs)):\n",
    "            inputs = XOR_inputs[i]\n",
    "            target = XOR_targets[i]\n",
    "            \n",
    "            h1_out = Hidden_Layer_1.forward(inputs)\n",
    "            h2_out = Hidden_Layer_2.forward(inputs)\n",
    "            hidden_outs = np.array([h1_out, h2_out])\n",
    "            final_out = Output_Neuron.forward(hidden_outs)\n",
    "            \n",
    "            total_error += (target - final_out)**2\n",
    "        print(f\"Epoch: {epoch + 1}, Total Squared Error: {total_error:.4f}\")\n",
    "\n",
    "# 5. Final Predictions (After Training)\n",
    "print(\"\\nFinal predictions after training:\")\n",
    "for i in range(len(XOR_inputs)):\n",
    "    inputs = XOR_inputs[i]\n",
    "    target = XOR_targets[i]\n",
    "    \n",
    "    h1_out = Hidden_Layer_1.forward(inputs)\n",
    "    h2_out = Hidden_Layer_2.forward(inputs)\n",
    "    hidden_outs = np.array([h1_out, h2_out])\n",
    "    final_out = Output_Neuron.forward(hidden_outs)\n",
    "    \n",
    "    print(f\"Inputs: {inputs}, Target: {target}, Prediction: {final_out:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7496bd51-c716-4705-89a3-6db4c6b1eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)  \n",
    "        self.fc2 = nn.Linear(4, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  \n",
    "        x = self.fc2(x)               \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f89c70-5968-47d0-80e0-fce35cbdbffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]) \n",
    "y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe2a250-cc68-4b3a-85d5-765ad1e0615d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1856256673.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    optimizer = optim.SGD(model.parameters(), lr=0.1)for epoch in range(100):\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Model, Define Loss Function and Optimizer\n",
    "model = SimpleNN()  \n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)for epoch in range(100):  \n",
    "    model.train() \n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)  \n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()  \n",
    "    loss.backward()  \n",
    "    optimizer.step()  \n",
    "\n",
    "    if (epoch + 1) % 10 == 0:  \n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45840e6b-f743-4b0a-b605-1c9184273d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "tensor([[-0.0549],\n",
      "        [ 0.0110],\n",
      "        [-0.0410],\n",
      "        [ 0.0321]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "with torch.no_grad(): \n",
    "    test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "    predictions = model(test_data) \n",
    "    print(f'Predictions:\\n{predictions}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
