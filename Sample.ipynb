{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37c21b91add6bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3152/1047917041.py:4: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  s=X.storage()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X=torch.tensor([1,2,3,4])\n",
    "print(torch.is_tensor(X))\n",
    "s=X.storage()\n",
    "print(torch.is_storage(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf230b4-24d8-4899-a030-d1c9d3d3c193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is prabhakar\n"
     ]
    }
   ],
   "source": [
    "print(\"This is prabhakar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe676756d07518f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(1,2,3,4)\n",
    "#Gives the Total Elements Present in  the Tensor... like If the matrix has size (2,3) give the Total Elements as 5\n",
    "torch.numel(a)\n",
    "a=torch.randn(2,3,4)\n",
    "torch.numel(a)\n",
    "b=torch.randn(2,3,4,5)\n",
    "torch.numel(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae4fc2a-d36b-44bd-99f5-1e8cf2f3e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccddeec6-5a11-408a-8ba3-f5043196c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [5, 9] | Output: 1\n",
      "Inputs: [2, -14] | Output: 0\n"
     ]
    }
   ],
   "source": [
    "# A simple neuron model\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs\n",
    "        weighted_sum = 0\n",
    "        for i in range(len(self.weights)):\n",
    "            weighted_sum += self.weights[i] * inputs[i]\n",
    "\n",
    "        # Add the bias\n",
    "        output = weighted_sum + self.bias\n",
    "        \n",
    "        # Apply an activation function (here we use a simple step function for clarity)\n",
    "        if output >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Example usage\n",
    "# Define the weights and bias for the neuron\n",
    "# Imagine we're trying to predict if a student passes a test based on\n",
    "# hours studied (input 1) and attendance rate (input 2)\n",
    "# Weight 1 (0.5) gives more importance to hours studied, Weight 2 (0.3) to attendance.\n",
    "# A bias of -2 means the student needs a certain minimum \"score\" to pass.\n",
    "neuron_weights = [0.5, 0.3]\n",
    "neuron_bias = -2.0\n",
    "\n",
    "# Create an instance of the Neuron\n",
    "my_neuron = Neuron(neuron_weights, neuron_bias)\n",
    "\n",
    "# Example 1: High hours studied (5) and high attendance (9)\n",
    "input_data_1 = [5, 9]\n",
    "output_1 = my_neuron.forward(input_data_1)\n",
    "print(f\"Inputs: {input_data_1} | Output: {output_1}\") # Output will be 1 (Pass)\n",
    "\n",
    "# Example 2: Low hours studied (2) and low attendance (4)\n",
    "input_data_2 = [2, -14]\n",
    "output_2 = my_neuron.forward(input_data_2)\n",
    "print(f\"Inputs: {input_data_2} | Output: {output_2}\") # Output will be 0 (Fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dae5014-5b8c-418d-acc4-27f45268ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Hidden Output is  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=np.array(weights)\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        #Calculate the weighted sum using a dot product\n",
    "        weighted_sum=np.dot(self.weights,inputs)+self.bais\n",
    "\n",
    "        if weighted_sum>=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "#Building Multilayer network\n",
    "Hidden_Layer_1=Neuron(weights=[3,-4],bais=3)\n",
    "Hidden_Layer_2=Neuron(weights=[5,-7],bais=4)\n",
    "input_data=np.array([4,6])\n",
    "Hidden_Output_1=Hidden_Layer_1.Forward(input_data)\n",
    "print(\"The First Hidden Output is \",Hidden_Output_1)\n",
    "# Hidden_Output_2=Hidden_Layer_2.Forward(Hidden_output_1)\n",
    "\n",
    "#Combines the output into new vector array\n",
    "# Hidden_Layer_Output=np.array([Hidden_Output_2])\n",
    "# print(f\"The inputs are {input_data} and Output are {Hidden_Layer_Output}\")\n",
    "# #Output Layer ::\n",
    "# #The output Layer takes the inputs of previous hidden Layers output and \n",
    "# Final_Neuron=Neuron(weights=[3,4],bais=-3)\n",
    "# Final_Output=Final_Neuron.Forward(Hidden_Layer_Output)\n",
    "# print(f\"The Final out put is {Final_Output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f52e6e2-81f1-48b4-85da-57a969fd0947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [5 9] | Hidden Layer Outputs: [1 1]\n",
      "Final Output of the network: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A simple neuron class using NumPy for efficiency\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum using a dot product (more efficient)\n",
    "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # Apply a simple step function as the activation\n",
    "        if weighted_sum >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# --- Building the Multi-Layer Network ---\n",
    "\n",
    "# --- Hidden Layer ---\n",
    "# Let's create two neurons for our hidden layer\n",
    "# These neurons will process the initial inputs (hours studied, attendance)\n",
    "# Neuron 1 focuses on one aspect (e.g., studying habits)\n",
    "hidden_neuron_1 = Neuron(weights=[0.5, 0.3], bias=-2.0)\n",
    "# Neuron 2 focuses on another aspect (e.g., participation and engagement)\n",
    "hidden_neuron_2 = Neuron(weights=[0.6, 0.4], bias=-3.0)\n",
    "\n",
    "# Example 1: High hours studied (5) and high attendance (9)\n",
    "input_data = np.array([5, 9])\n",
    "\n",
    "# Calculate the output of each neuron in the hidden layer\n",
    "# These outputs will be the new inputs for the next layer\n",
    "hidden_output_1 = hidden_neuron_1.forward(input_data)\n",
    "hidden_output_2 = hidden_neuron_2.forward(input_data)\n",
    "\n",
    "# Combine the outputs into a new input vector for the next layer\n",
    "hidden_layer_output = np.array([hidden_output_1, hidden_output_2])\n",
    "print(f\"Input: {input_data} | Hidden Layer Outputs: {hidden_layer_output}\")\n",
    "\n",
    "# --- Output Layer ---\n",
    "# Now, let's create a final output neuron that takes the hidden layer's outputs as its input\n",
    "# The weights here decide how much importance the network places on the outputs of the hidden neurons.\n",
    "# This neuron predicts the final outcome (Pass or Fail)\n",
    "output_neuron = Neuron(weights=[0.7, 0.8], bias=-1.0)\n",
    "final_output = output_neuron.forward(hidden_layer_output)\n",
    "print(f\"Final Output of the network: {final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b758926-99d3-4b64-ab60-823a2e6ca97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1 Output: [1 1]\n",
      "Hidden Layer 2 Output: [1 1]\n",
      "Final Output of the Network: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# A simple neuron class using NumPy\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weighted_sum = np.dot(self.weights, inputs) + self.bias\n",
    "        # Simple step activation\n",
    "        return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ First Hidden Layer (2 neurons)\n",
    "hidden1_neuron1 = Neuron(weights=[0.5, 0.3], bias=-2.0)\n",
    "hidden1_neuron2 = Neuron(weights=[0.6, 0.4], bias=-3.0)\n",
    "\n",
    "# Input data\n",
    "input_data = np.array([5, 9])\n",
    "\n",
    "# Forward through first hidden layer\n",
    "h1_out1 = hidden1_neuron1.forward(input_data)\n",
    "h1_out2 = hidden1_neuron2.forward(input_data)\n",
    "hidden1_output = np.array([h1_out1, h1_out2])\n",
    "print(f\"Hidden Layer 1 Output: {hidden1_output}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Second Hidden Layer (2 neurons)\n",
    "# These neurons take the outputs of the first hidden layer as their inputs\n",
    "hidden2_neuron1 = Neuron(weights=[0.2, 0.4], bias=-0.5)\n",
    "hidden2_neuron2 = Neuron(weights=[0.3, 0.7], bias=-1.0)\n",
    "\n",
    "# Forward through second hidden layer\n",
    "h2_out1 = hidden2_neuron1.forward(hidden1_output)\n",
    "h2_out2 = hidden2_neuron2.forward(hidden1_output)\n",
    "hidden2_output = np.array([h2_out1, h2_out2])\n",
    "print(f\"Hidden Layer 2 Output: {hidden2_output}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Output Layer (final neuron)\n",
    "output_neuron = Neuron(weights=[0.7, 0.8], bias=-1.0)\n",
    "final_output = output_neuron.forward(hidden2_output)\n",
    "print(f\"Final Output of the Network: {final_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c76045-aa7d-4030-af05-3364ada13313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output BEFORE backpropagation: 0.5381997566506256\n",
      "Output AFTER backpropagation: 0.5653404376286888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activation + derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias, lr=0.1):\n",
    "        self.weights = np.array(weights, dtype=float)\n",
    "        self.bias = float(bias)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = np.array(inputs, dtype=float)\n",
    "        self.weighted_sum = np.dot(self.weights, self.inputs) + self.bias\n",
    "        self.output = sigmoid(self.weighted_sum)                                                                     \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dloss_dout):\n",
    "        d_out_d_sum = sigmoid_derivative(self.output)\n",
    "        dloss_dsum = dloss_dout * d_out_d_sum\n",
    "        dloss_dweights = dloss_dsum * self.inputs\n",
    "        dloss_dbias = dloss_dsum\n",
    "        dloss_dinputs = dloss_dsum * self.weights\n",
    "        # update params\n",
    "        self.weights -= self.lr * dloss_dweights\n",
    "        self.bias -= self.lr * dloss_dbias\n",
    "        return dloss_dinputs\n",
    "\n",
    "# create network with some chosen values\n",
    "Hidden_Layer_1 = Neuron(weights=[0.5, -0.2], bias=0.1, lr=0.5)\n",
    "Hidden_Layer_2 = Neuron(weights=[0.3, 0.8], bias=-0.3, lr=0.5)\n",
    "Output_Neuron = Neuron(weights=[-0.7, 0.9], bias=0.05, lr=0.5)\n",
    "\n",
    "inputs = np.array([1.0, 0.5])  # sample input\n",
    "target = 1.0                   # desired output\n",
    "\n",
    "# ---- Forward pass (before training) ----\n",
    "h1_out = Hidden_Layer_1.forward(inputs)\n",
    "h2_out = Hidden_Layer_2.forward(inputs)\n",
    "hidden_outs = np.array([h1_out, h2_out])\n",
    "final_out = Output_Neuron.forward(hidden_outs)\n",
    "\n",
    "print(\"Output BEFORE backpropagation:\", final_out)\n",
    "\n",
    "# ---- Compute loss gradient and backprop ----\n",
    "dloss_dfinal = -(target - final_out)  # dL/dOut for output neuron\n",
    "dloss_dhidden = Output_Neuron.backward(dloss_dfinal)\n",
    "Hidden_Layer_1.backward(dloss_dhidden[0])\n",
    "Hidden_Layer_2.backward(dloss_dhidden[1])\n",
    "\n",
    "# ---- Forward pass (after weight update) ----\n",
    "h1_out = Hidden_Layer_1.forward(inputs)\n",
    "h2_out = Hidden_Layer_2.forward(inputs)\n",
    "hidden_outs = np.array([h1_out, h2_out])\n",
    "final_out_after = Output_Neuron.forward(hidden_outs)\n",
    "\n",
    "print(\"Output AFTER backpropagation:\", final_out_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bace84c4-eaf9-40e2-845d-2e03a4c05b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n",
      "The inputs are [3, -5] and Outputs are 0\n"
     ]
    }
   ],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,weights,bais):\n",
    "        self.weights=weights\n",
    "        self.bais=bais\n",
    "    def Forward(self,inputs):\n",
    "        self.inputs=inputs\n",
    "        weighted_sum=0\n",
    "        for i in range(len(self.inputs)):\n",
    "            weighted_sum+=self.weights[i]*self.inputs[i]\n",
    "        output=weighted_sum+self.bais\n",
    "        print(output)\n",
    "        if output>=0:\n",
    "            return 1\n",
    "\n",
    "        else:\n",
    "            return 0\n",
    "weights=[3,5]\n",
    "bais=4\n",
    "My_Neuron=Neuron(weights,bais)\n",
    "inputs=[3,-5]\n",
    "output=My_Neuron.Forward(inputs)\n",
    "print(f\"The inputs are {inputs} and Outputs are {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be572cab-4f3d-4d7f-8ee3-537f9c8215de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is Prabhakar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
